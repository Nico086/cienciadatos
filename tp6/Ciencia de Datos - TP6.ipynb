{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciencia de Datos - TP6\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Ambroa, Nicolás - 229/13 - ambroanicolas@hotmail.com\n",
    "- Gaustein, Diego - 586/09 - diego@gaustein.com.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "from string import punctuation, whitespace\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from lxml import html\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from operator import itemgetter\n",
    "import gensim\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = frozenset(stopwords.words('english'))\n",
    "\n",
    "def load_or_call(func):\n",
    "    \"\"\" Decorador auxiliar para cachear resultados en un .pickle. \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        filename = '{}-{}-{}.pickle'.format(func.__name__, str(args), str(kwargs))\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            res = func(*args, **kwargs)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(res, f)\n",
    "            return res\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Levantar el corpus AP, separando cada noticia como un elemento distinto en un diccionario `(<DOCNO>: <TEXT>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 2250 articulos\n"
     ]
    }
   ],
   "source": [
    "parsed = html.parse('ap/ap.txt')\n",
    "documents = {}\n",
    "for document in parsed.iter('doc'):\n",
    "    docno, article = document.getchildren()\n",
    "    documents[docno.text.strip()] = article.text.strip()\n",
    "\n",
    "print('Cargados', len(documents), 'articulos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calcular el tamaño del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548169 palabras, 41090 palabras distintas.\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN  # El default es NOUN\n",
    "\n",
    "def get_words_for_document(document):\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tagged_sentence = pos_tag(sentence.split())\n",
    "        for word, pos in tagged_sentence:\n",
    "            # Lematizar\n",
    "            word = lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos))\n",
    "            # Strip punctuation\n",
    "            word = word.strip(punctuation)\n",
    "            # Lowercase\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Skip stopwords and punctuation\n",
    "            if len(word) > 1 and word not in stopwords:\n",
    "                yield word\n",
    "\n",
    "@load_or_call\n",
    "def get_word_count():\n",
    "    c = Counter()\n",
    "    for document in documents.values():\n",
    "        c.update(get_words_for_document(document))\n",
    "    return c\n",
    "        \n",
    "c = get_word_count()\n",
    "cantidad_de_palabras = sum(c.values())\n",
    "print(cantidad_de_palabras, 'palabras,', len(c), 'palabras distintas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Para las 500 palabras con más apariciones, calcular el par más asociado según la medida presentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 5 pares de palabras más asociadas según la medida de información mutua son: \n",
      "\n",
      "* index value con información mutua: 1838.0320849868358\n",
      "* conference news con información mutua: 1421.3935232619224\n",
      "* 30 average con información mutua: 1384.3817325708626\n",
      "* executive chief con información mutua: 1228.8735910193982\n",
      "* fell index con información mutua: 1106.339421879901\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Para cada palabra, aproximamos p(palabra) con su frecuencia relativa según el counter.\n",
    "Tomamos las 500 con más apariciones y recorremos el texto. Cada vez que encontremos una de estas miramos en una\n",
    "ventana (n=8) cuando aparece alguna de las otras, y le sumamos uno.\n",
    "Esta es nuestra estimación de p(palabra1, palabra2). Finalmente calculamos la información mutua e imprimimos los\n",
    "pares más asociados.\n",
    "\"\"\"\n",
    "\n",
    "def mirar_ventana_y_buscar(palabra_a_asociar, palabras, palabras_a_buscar, indice_inicial, longitud_de_ventana, asociaciones):\n",
    "    # Arreglo los índices para no pasarme en caso de una ventana muy cercana a los extremos de la lista.\n",
    "    if indice_inicial - longitud_de_ventana < 0:\n",
    "        indice_inicial = 0\n",
    "    else:\n",
    "        indice_inicial -= longitud_de_ventana\n",
    "    if indice_inicial + longitud_de_ventana > len(palabras):\n",
    "        indice_final = len(palabras)\n",
    "    else:\n",
    "        indice_final = indice_inicial + longitud_de_ventana\n",
    "       \n",
    "    # Recorro los indices, buscando palabras a buscar que no sean la palabra a asociar.\n",
    "    for indice in range(indice_inicial, indice_final):\n",
    "        if palabras[indice] in palabras_a_buscar and palabras[indice] != palabra_a_asociar:\n",
    "            subindice_dict = str(palabra_a_asociar) + \" \" +str(palabras[indice])\n",
    "            # Si la encontre, aumento en uno la asociación entre ambas palabras.\n",
    "            try:\n",
    "                asociaciones[subindice_dict] += 1\n",
    "            except IndexError:\n",
    "                asociaciones[subindice_dict] = 1\n",
    "\n",
    "@load_or_call\n",
    "def contar_asociaciones_de_palabras():\n",
    "    contador_asociaciones_de_palabras = Counter()\n",
    "    # Recorremos el texto.\n",
    "    for document in documents.values():\n",
    "        palabras = document.split()\n",
    "        for indice, palabra in enumerate(palabras):\n",
    "            #Cada vez que encontremos una de ellas, miramos en una ventana de n=8.\n",
    "            if palabra in palabras_con_mas_apariciones:\n",
    "                mirar_ventana_y_buscar(palabra, palabras, palabras_con_mas_apariciones, indice, 8, contador_asociaciones_de_palabras)\n",
    "    return contador_asociaciones_de_palabras\n",
    "\n",
    "# Tomamos las 500 palabras con más apariciones.\n",
    "palabras_y_apariciones = c.most_common(500)\n",
    "palabras_con_mas_apariciones = [tupla[0] for tupla in palabras_y_apariciones]\n",
    "\n",
    "asociaciones_de_palabras = contar_asociaciones_de_palabras()\n",
    "suma_de_asociaciones_totales = sum(asociaciones_de_palabras.values())\n",
    "informaciones_mutuas = Counter()\n",
    "# Recorremos los pares y calculamos información mutua para cada uno.\n",
    "for palabras, asociaciones in asociaciones_de_palabras.items():\n",
    "    palabras_parseadas = palabras.split(\" \")\n",
    "    palabra_x = palabras_parseadas[0]\n",
    "    palabra_y = palabras_parseadas[1]\n",
    "    proba_x = c[palabra_x] / float(cantidad_de_palabras)\n",
    "    proba_y = c[palabra_y] / float(cantidad_de_palabras)\n",
    "    proba_conj_x_y  = asociaciones_de_palabras[palabras] / suma_de_asociaciones_totales\n",
    "    inf_mutua_x_y = proba_conj_x_y / (proba_x*proba_y)\n",
    "    informaciones_mutuas[palabras] = inf_mutua_x_y\n",
    "\n",
    "# Finalmente imprimimos los 5 pares más asociados según la información mutua\n",
    "print(\"Los 5 pares de palabras más asociadas según la medida de información mutua son: \\n\")\n",
    "for palabra_y_info_mutua in informaciones_mutuas.most_common(5):\n",
    "    print(\"* {0} con información mutua: {1}\".format(palabra_y_info_mutua[0], palabra_y_info_mutua[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Word embeddings, distancia semántica y Word-Net\n",
    "\n",
    "### a) Utilizando el test WordSim353 , comparar el rendimiento entre LSA y Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Word2Vec y LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Voy a ejecutar éste código solamente si no lo ejecute antes.\n",
    "if not os.path.exists('obtener_metrica_de_similitud_por_modelo-()-{}.pickle'):\n",
    "    \n",
    "    # Primero el modelo Word2Vec.\n",
    "    # Para utilizar éste modelo, se debe descargar el link puesto debajo y ubicarlo en la carpeta source del proyecto.\n",
    "    # Para el model de Google: http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    # Luego, el modelo LSA.\n",
    "    # Primero, tenemos que armar el corpus para LSA adaptándonos al formato TF-IDF.\n",
    "    oraciones = []\n",
    "    # Pasamos todos los documentos del corpus a la lista oraciones, limpiando en el proceso.\n",
    "    directorio = os.fsencode('brown/')\n",
    "    for archivo in os.listdir(directorio):\n",
    "        filename = os.fsdecode(archivo)\n",
    "        nombre_completo = 'brown/' + filename\n",
    "        with open(nombre_completo) as f:\n",
    "            lineas = f.read().splitlines()\n",
    "            lineas_limpias = [\" \".join(word.split(\"/\")[0] for word in s.split()) for s in lineas]\n",
    "            oraciones += lineas_limpias\n",
    "\n",
    "    # Sacamos palabras que queremos ignorar, espacios vacíos, etc. \n",
    "    ignorables = list('for a of the and to in if from by'.split()) + list(punctuation) + list(whitespace) + [[]]\n",
    "\n",
    "    # Armamos una gran lista de palabras por documento\n",
    "    palabras = [[word for word in document.lower().split() if word not in ignorables]\n",
    "                 for document in oraciones if document]\n",
    "\n",
    "    lsa_dicionario = gensim.corpora.Dictionary(palabras)\n",
    "    # doc2bow() cuenta la cantidad de apariciones de c/palabra, transforma la palabra a su entero_id y devuelve\n",
    "    # el resultado como un vector esparso.\n",
    "    lsa_corpus = [lsa_dicionario.doc2bow(palabra) for palabra in palabras]\n",
    "\n",
    "    # Inicializamos modelo TF-IDF.\n",
    "    modelo_tfidf = gensim.models.TfidfModel(lsa_corpus)\n",
    "    corpus_tfidf = modelo_tfidf[lsa_corpus]\n",
    "    if not os.path.exists('lsi_model'):\n",
    "        # Finalmente, tenemos el modelo LSI (LSA).\n",
    "        modelo_lsi = gensim.models.LsiModel(corpus_tfidf, id2word=lsa_dicionario, num_topics=300)\n",
    "        modelo_lsi.save('lsi_model')\n",
    "    else:\n",
    "        modelo_lsi = gensim.models.LsiModel.load('lsi_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testeo Similitud con la medida conocida como Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_similitud_lsa(palabra_1, palabra_2):\n",
    "    doc = palabra_1 + \" \" + palabra_2\n",
    "    similitud_query = lsa_dicionario.doc2bow(doc.lower().split())\n",
    "    similitud_lsi = modelo_lsi[similitud_query]\n",
    "    maximo = 0\n",
    "    for tupla in similitud_lsi:\n",
    "        if tupla[1] > maximo:\n",
    "            maximo = tupla[1]\n",
    "    return maximo\n",
    "\n",
    "@load_or_call\n",
    "def obtener_metrica_de_similitud_por_modelo():\n",
    "    similitudes_por_modelo = {'WordSim353': {}, 'Word2Vec': {}, 'LSA': {}}\n",
    "    similitud_test_filename = 'wordsimtest/wordsim_similarity_goldstandard.txt'\n",
    "    with open(similitud_test_filename, 'rb') as similitud_test_file:\n",
    "        for line in similitud_test_file.readlines():\n",
    "            # Parseo línea de palabras similares.\n",
    "            linea = line.decode('UTF-8').split()\n",
    "            primera_palabra = linea[0]\n",
    "            segunda_palabra = linea[1]\n",
    "            subindice = primera_palabra + \" \" + segunda_palabra\n",
    "            # Veo similaridad en WordSim353.\n",
    "            similitudes_por_modelo['WordSim353'][subindice] = linea[2]\n",
    "            # Veo similarity en Word2Vec.\n",
    "            similitudes_por_modelo['Word2Vec'][subindice] = word2vec_model.wv.similarity(primera_palabra, segunda_palabra)\n",
    "            # ToDo: agregar la parte de LSA\n",
    "            similitudes_por_modelo['LSA'][subindice] = obtener_similitud_lsa(primera_palabra, segunda_palabra)\n",
    "            return similitudes_por_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "similitudes_por_modelo = obtener_metrica_de_similitud_por_modelo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud para Word2Vec en el caso de tiger cat es: 0.5172961902020222\n"
     ]
    }
   ],
   "source": [
    "print(\"La similitud para Word2Vec en el caso de tiger cat es: {0}\".format(similitudes_por_modelo['Word2Vec']['tiger cat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud para LSA en el caso de tiger cat es: 0.004632709012058887\n"
     ]
    }
   ],
   "source": [
    "print(\"La similitud para LSA en el caso de tiger cat es: {0}\".format(similitudes_por_modelo['LSA']['tiger cat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recortamos el input de similitudes para que no ensucie el documento. En general, y como se aprecia en el ejemplo, el modelo Word2Vec presenta resultados con similitudes más altas en sus conjuntos de palabras comparado con el modelo LSA. Creemos que ésto se debe a que el set de entrenamiento del modelo Word2Vec provisto por Google es mucho más robusto y amplio que el brown corpus de LSA (que data de 1961)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
