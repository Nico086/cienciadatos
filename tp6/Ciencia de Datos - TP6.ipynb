{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciencia de Datos - TP6\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Ambroa, Nicolás - 229/13 - ambroanicolas@hotmail.com\n",
    "- Gaustein, Diego - 586/09 - diego@gaustein.com.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from lxml import html\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import gensim\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = frozenset(stopwords.words('english'))\n",
    "\n",
    "def load_or_call(func):\n",
    "    \"\"\" Decorador auxiliar para cachear resultados en un .pickle. \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        filename = '{}-{}-{}.pickle'.format(func.__name__, str(args), str(kwargs))\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            res = func(*args, **kwargs)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(res, f)\n",
    "            return res\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Levantar el corpus AP, separando cada noticia como un elemento distinto en un diccionario `(<DOCNO>: <TEXT>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 2250 articulos\n"
     ]
    }
   ],
   "source": [
    "parsed = html.parse('ap/ap.txt')\n",
    "documents = {}\n",
    "for document in parsed.iter('doc'):\n",
    "    docno, article = document.getchildren()\n",
    "    documents[docno.text.strip()] = article.text.strip()\n",
    "\n",
    "print('Cargados', len(documents), 'articulos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calcular el tamaño del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548169 palabras, 41090 palabras distintas.\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN  # El default es NOUN\n",
    "\n",
    "def get_words_for_document(document):\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tagged_sentence = pos_tag(sentence.split())\n",
    "        for word, pos in tagged_sentence:\n",
    "            # Lematizar\n",
    "            word = lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos))\n",
    "            # Strip punctuation\n",
    "            word = word.strip(punctuation)\n",
    "            # Lowercase\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Skip stopwords and punctuation\n",
    "            if len(word) > 1 and word not in stopwords:\n",
    "                yield word\n",
    "\n",
    "@load_or_call\n",
    "def get_word_count():\n",
    "    c = Counter()\n",
    "    for document in documents.values():\n",
    "        c.update(get_words_for_document(document))\n",
    "    return c\n",
    "        \n",
    "c = get_word_count()\n",
    "cantidad_de_palabras = sum(c.values())\n",
    "print(cantidad_de_palabras, 'palabras,', len(c), 'palabras distintas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Para las 500 palabras con más apariciones, calcular el par más asociado según la medida presentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 5 pares de palabras más asociadas según la medida de información mutua son: \n",
      "\n",
      "* index value con información mutua: 1838.0320849868358\n",
      "* conference news con información mutua: 1421.3935232619224\n",
      "* 30 average con información mutua: 1384.3817325708626\n",
      "* executive chief con información mutua: 1228.8735910193982\n",
      "* fell index con información mutua: 1106.339421879901\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Para cada palabra, aproximamos p(palabra) con su frecuencia relativa según el counter.\n",
    "Tomamos las 500 con más apariciones y recorremos el texto. Cada vez que encontremos una de estas miramos en una\n",
    "ventana (n=8) cuando aparece alguna de las otras, y le sumamos uno.\n",
    "Esta es nuestra estimación de p(palabra1, palabra2). Finalmente calculamos la información mutua e imprimimos los\n",
    "pares más asociados.\n",
    "\"\"\"\n",
    "\n",
    "def mirar_ventana_y_buscar(palabra_a_asociar, palabras, palabras_a_buscar, indice_inicial, longitud_de_ventana, asociaciones):\n",
    "    # Arreglo los índices para no pasarme en caso de una ventana muy cercana a los extremos de la lista.\n",
    "    if indice_inicial - longitud_de_ventana < 0:\n",
    "        indice_inicial = 0\n",
    "    else:\n",
    "        indice_inicial -= longitud_de_ventana\n",
    "    if indice_inicial + longitud_de_ventana > len(palabras):\n",
    "        indice_final = len(palabras)\n",
    "    else:\n",
    "        indice_final = indice_inicial + longitud_de_ventana\n",
    "       \n",
    "    # Recorro los indices, buscando palabras a buscar que no sean la palabra a asociar.\n",
    "    for indice in range(indice_inicial, indice_final):\n",
    "        if palabras[indice] in palabras_a_buscar and palabras[indice] != palabra_a_asociar:\n",
    "            subindice_dict = str(palabra_a_asociar) + \" \" +str(palabras[indice])\n",
    "            # Si la encontre, aumento en uno la asociación entre ambas palabras.\n",
    "            try:\n",
    "                asociaciones[subindice_dict] += 1\n",
    "            except IndexError:\n",
    "                asociaciones[subindice_dict] = 1\n",
    "\n",
    "@load_or_call\n",
    "def contar_asociaciones_de_palabras():\n",
    "    contador_asociaciones_de_palabras = Counter()\n",
    "    # Recorremos el texto.\n",
    "    for document in documents.values():\n",
    "        palabras = document.split()\n",
    "        for indice, palabra in enumerate(palabras):\n",
    "            #Cada vez que encontremos una de ellas, miramos en una ventana de n=8.\n",
    "            if palabra in palabras_con_mas_apariciones:\n",
    "                mirar_ventana_y_buscar(palabra, palabras, palabras_con_mas_apariciones, indice, 8, contador_asociaciones_de_palabras)\n",
    "    return contador_asociaciones_de_palabras\n",
    "\n",
    "# Tomamos las 500 palabras con más apariciones.\n",
    "palabras_y_apariciones = c.most_common(500)\n",
    "palabras_con_mas_apariciones = [tupla[0] for tupla in palabras_y_apariciones]\n",
    "\n",
    "asociaciones_de_palabras = contar_asociaciones_de_palabras()\n",
    "suma_de_asociaciones_totales = sum(asociaciones_de_palabras.values())\n",
    "informaciones_mutuas = Counter()\n",
    "# Recorremos los pares y calculamos información mutua para cada uno.\n",
    "for palabras, asociaciones in asociaciones_de_palabras.items():\n",
    "    palabras_parseadas = palabras.split(\" \")\n",
    "    palabra_x = palabras_parseadas[0]\n",
    "    palabra_y = palabras_parseadas[1]\n",
    "    proba_x = c[palabra_x] / float(cantidad_de_palabras)\n",
    "    proba_y = c[palabra_y] / float(cantidad_de_palabras)\n",
    "    proba_conj_x_y  = asociaciones_de_palabras[palabras] / suma_de_asociaciones_totales\n",
    "    inf_mutua_x_y = proba_conj_x_y / (proba_x*proba_y)\n",
    "    informaciones_mutuas[palabras] = inf_mutua_x_y\n",
    "\n",
    "# Finalmente imprimimos los 5 pares más asociados según la información mutua\n",
    "print(\"Los 5 pares de palabras más asociadas según la medida de información mutua son: \\n\")\n",
    "for palabra_y_info_mutua in informaciones_mutuas.most_common(5):\n",
    "    print(\"* {0} con información mutua: {1}\".format(palabra_y_info_mutua[0], palabra_y_info_mutua[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Word embeddings, distancia semántica y Word-Net\n",
    "\n",
    "### a) Utilizando el test WordSim353 , comparar el rendimiento entre LSA y Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docs para usar word2vec\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "# Para el model de Google: http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "@load_or_call\n",
    "def obtener_oraciones_de_corpus():\n",
    "    oraciones = []\n",
    "    for texto in documents.values():\n",
    "        texto_por_oraciones = sent_tokenize(texto)\n",
    "        oraciones.append(texto_por_oraciones)\n",
    "    return oraciones\n",
    "\n",
    "oraciones = obtener_oraciones_de_corpus()\n",
    "word2vec_model = model = gensim.models.KeyedVectors.load_word2vec_format('/word2vec_model/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "word2vec_model = gensim.models.Word2Vec(oraciones, iter=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d28d51b1c48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Document-Term Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdtMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moraciones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeaturenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Buscar corpus en http://www.nltk.org/data.html creo que le brown corpus sirve.\n",
    "# Para instalar los corpus: sudo python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "lsa_corpus = 'nombre_del_archivo_donde_esta_el_corpus'\n",
    "#Document-Term Matrix\n",
    "cv = CountVectorizer(input='filename',strip_accents='ascii')\n",
    "dtMatrix = cv.fit_transform(lsa_corpus).toarray()\n",
    "print (dtMatrix.shape)\n",
    "featurenames = cv.get_feature_names()\n",
    "print (featurenames)\n",
    "\n",
    "#Tf-idf Transformation\n",
    "tfidf = TfidfTransformer()\n",
    "tfidfMatrix = tfidf.fit_transform(dtMatrix).toarray()\n",
    "print (tfidfMatrix.shape)\n",
    "\n",
    "#SVD\n",
    "#n_components is recommended to be 100 by Sklearn Documentation for LSA\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "svd = TruncatedSVD(n_components = 100)\n",
    "svdMatrix = svd.fit_transform(tfidfMatrix)\n",
    "\n",
    "print (svdMatrix)\n",
    "\n",
    "#Cosine-Similarity\n",
    "#cosine = cosine_similarity(svdMatrix[1], svdMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testeo Similitud y Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá comparo las similitudes entre las palabras de wordsimtest test similitud\n",
    "# EJ: model.similarity('woman', 'man')\n",
    "similarities = {'WordSim353': {}, 'Word2Vec': {}, 'LSA': {}}\n",
    "similarity_test_filename = 'wordsimtest/wordsim_similarity_goldstandard.txt'\n",
    "with open(similarity_test_filename, 'rb') as similarity_test_file:\n",
    "    for line in similarity_test_file.readlines():\n",
    "        decoded_line = line.decode('UTF-8').split()\n",
    "        similiarity_from_test = decoded_line[2]\n",
    "        first_word = decoded_line[0]\n",
    "        second_word = decoded_line[1]\n",
    "        subindex = first_word + \" \" + second_word\n",
    "        similarity['WordSim353'][subindex] = similiarity_from_test\n",
    "        # ToDo: agregar la parte de LSA\n",
    "        # De word2vec falta hacer:\n",
    "        # word2vec_similarity = word2vec_model.similarity(first_word, second_word)\n",
    "        # similarities['Word2Vec'][subindex] = word2vec_similarity\n",
    "        \n",
    "# ToDo: Lo mismo de arriba pero para relatedness?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
