{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciencia de Datos - TP5\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Ambroa, Nicolás - 229/13 - ambroanicolas@hotmail.com\n",
    "- Gaustein, Diego - 586/09 - diego@gaustein.com.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from lxml import html\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = frozenset(stopwords.words('english'))\n",
    "\n",
    "def load_or_call(func):\n",
    "    \"\"\" Decorador auxiliar para cachear resultados en un .pickle. \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        filename = '{}-{}-{}.pickle'.format(func.__name__, str(args), str(kwargs))\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            res = func(*args, **kwargs)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(res, f)\n",
    "            return res\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Levantar el corpus AP, separando cada noticia como un elemento distinto en un diccionario `(<DOCNO>: <TEXT>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 2250 articulos\n"
     ]
    }
   ],
   "source": [
    "parsed = html.parse('ap/ap.txt')\n",
    "documents = {}\n",
    "for document in parsed.iter('doc'):\n",
    "    docno, article = document.getchildren()\n",
    "    documents[docno.text.strip()] = article.text.strip()\n",
    "\n",
    "print('Cargados', len(documents), 'articulos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calcular el tamaño del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548169 palabras, 41090 palabras distintas.\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN  # El default es NOUN\n",
    "\n",
    "def get_words_for_document(document):\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tagged_sentence = pos_tag(sentence.split())\n",
    "        for word, pos in tagged_sentence:\n",
    "            # Lematizar\n",
    "            word = lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos))\n",
    "            # Strip punctuation\n",
    "            word = word.strip(punctuation)\n",
    "            # Lowercase\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Skip stopwords and punctuation\n",
    "            if len(word) > 1 and word not in stopwords:\n",
    "                yield word\n",
    "\n",
    "@load_or_call\n",
    "def get_word_count():\n",
    "    c = Counter()\n",
    "    for document in documents.values():\n",
    "        c.update(get_words_for_document(document))\n",
    "    return c\n",
    "        \n",
    "c = get_word_count()\n",
    "cantidad_de_palabras = sum(c.values())\n",
    "print(cantidad_de_palabras, 'palabras,', len(c), 'palabras distintas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Para las 500 palabras con más apariciones, calcular el par más asociado según la medida presentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Para cada palabra, aproximamos p(palabra) con su frecuencia relativa según el counter.\n",
    "Tomamos las 500 con más apariciones y recorremos el texto. Cada vez que encontremos una de estas miramos en una\n",
    "ventana (n=8) cuando aparece alguna de las otras, y le sumamos uno.\n",
    "Esta es nuestra estimación de p(palabra1, palabra2). Finalmente calculamos la información mutua e imprimimos los\n",
    "pares más asociados.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
